{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76f63cc-4748-4568-aa66-de24926caba0",
   "metadata": {},
   "source": [
    "**Web-scraping football game data from the English Premier League** \n",
    "\n",
    "This first section of code follows 3 broad steps to extract data on premier league matches across multiple seasons:\n",
    "\n",
    "For each season, the code does the following:\n",
    "\n",
    "1. Go to [this EPL data page](https://fbref.com/en/comps/9/Premier-League-Stats) and find the premier league table from that season to extract the URL links for every club in the league.\n",
    "\n",
    "2. For each team in the league in this given season, go into the team's URL link and:\n",
    "\n",
    "    *2a) Scrape **basic match data** from the 'Scores & Fixtures' table. This data includes the date, matchweek, opponent, venue, score, possession and expected goals for and against.*\n",
    "    \n",
    "    *2b) Scrape **shooting data** from the 'Shooting' table. This provides us with useful information on shots and shots on target, both for and against each team.*\n",
    "    \n",
    "3. Once this is repeated across multiple seasons and across all teams in the premier league in that season, all the above information is then merged into a single dataframe. \n",
    "\n",
    "The second section of this code follows a similar, albeit simpler process to go to the same website and extract the final premier leage standings table for each season - this is important as it provides a useful feature for our model to indicate the relative strength of a team based on their league position in the previous season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da12f0ce-f9db-490b-bf68-486bb810bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup \n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# let's configure python to display to 1 decimpal places\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "# conigure pandas to display all columns so we can view the whole dataset\n",
    "pd.set_option(\"display.max.columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a97854-4188-48e1-8180-f51f474d35aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2022, 2021, 2020, 2019, 2018, 2017]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a list of years (seasons) to repeat the data scraping loop over\n",
    "years = list(range(2022, 2016, -1)) # we only include data post-2017 because crucual predictors like expected goals are not available before then\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6782da9d-b9ae-45ae-b2a6-38e6fae7fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty object to store the data\n",
    "all_matches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10f02021-ab36-4a73-9264-43a028032fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the URL with the premier league table from the latest season (2022-23)\n",
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0ad043-53e5-4cba-8502-b2354db36496",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m## COLLECT URLS FOR EACH INDIVIDUAL TEAM\u001b[39;00m\n\u001b[0;32m      7\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(standings_data\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;66;03m# use BeautifulSoup to parse the text object received from the URL\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m standings_table \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtable.stats_table\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# select the stats table object from the parsed object\u001b[39;00m\n\u001b[0;32m     11\u001b[0m links \u001b[38;5;241m=\u001b[39m [l\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m standings_table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;66;03m# find all objects starting with '<a' and extract href objects\u001b[39;00m\n\u001b[0;32m     13\u001b[0m links \u001b[38;5;241m=\u001b[39m [l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m links \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/squads/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m l] \u001b[38;5;66;03m# collect all links that contain '/squads' in their URLs\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    \n",
    "    standings_data = requests.get(standings_url) # send GET request to the standing URL\n",
    "    \n",
    "    ## COLLECT URLS FOR EACH INDIVIDUAL TEAM\n",
    "                        \n",
    "    soup = BeautifulSoup(standings_data.text) # use BeautifulSoup to parse the text object received from the URL\n",
    "    \n",
    "    standings_table = soup.select('table.stats_table')[0] # select the stats table object from the parsed object\n",
    "\n",
    "    links = [l.get(\"href\") for l in standings_table.find_all('a')] # find all objects starting with '<a' and extract href objects\n",
    "    \n",
    "    links = [l for l in links if '/squads/' in l] # collect all links that contain '/squads' in their URLs\n",
    "    \n",
    "    team_urls = [f\"https://fbref.com{l}\" for l in links] # complete the URLs by adding website opening text on the front\n",
    "    \n",
    "    previous_season = soup.select(\"a.prev\")[0].get(\"href\") # collect the href link for the previous season\n",
    "    \n",
    "    standings_url = f\"https://fbref.com{previous_season}\" # complete the previous season url to set up next stage of the loop once data on all teams are collected\n",
    "    \n",
    "    for team_url in team_urls: # for each URL in the team URLs\n",
    "        \n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \") # obtain the team name from the URL\n",
    "        \n",
    "        team_data = requests.get(team_url) # send GET request to the team's URL\n",
    "        \n",
    "        ## GET BASIC MATCH DATA FROM TEAM URL\n",
    "        \n",
    "        matches = pd.read_html(team_data.text, match=\"Scores & Fixtures\")[0] # read the matches data from the scores and fixtures table\n",
    "        \n",
    "        matches.columns = map(str.lower, matches.columns) # make columns lower case\n",
    "        \n",
    "        matches.columns = matches.columns.str.replace(' ', '_') # replace spaces in column names\n",
    "        \n",
    "        ## GET SHOOTING DATA FROM TEAM URL\n",
    "        \n",
    "        soup = BeautifulSoup(team_data.text) # parse the team_url GET request\n",
    "        \n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')] # collect the links\n",
    "        \n",
    "        shooting_links = [l for l in links if l and 'all_comps/shooting/' in l] # collect the links that contain the shooting variables\n",
    "        \n",
    "        all_shooting_data = pd.read_html(f\"https://fbref.com{shooting_links[0]}\") # read all the tables from the shooting URL into a pandas data frame\n",
    "        \n",
    "        shooting_data_for = all_shooting_data[0] # Collect the 'for' data (i.e., for the team in question)\n",
    "        \n",
    "        shooting_data_for.columns = shooting_data_for.columns.droplevel() # now drop the top level above the standard columns\n",
    "              \n",
    "        shooting_data_for.columns = map(str.lower, shooting_data_for.columns) # make all the column names lower case       \n",
    "        \n",
    "        shooting_data_for.columns = shooting_data_for.columns.str.replace(' ', '_') # replace any spaces with an underscore\n",
    "         \n",
    "        shooting_data_against = all_shooting_data[-1] # Collect the 'against' data\n",
    "               \n",
    "        shooting_data_against.columns = shooting_data_against.columns.droplevel() # now drop the top level above the standard columns\n",
    "               \n",
    "        shooting_data_against.columns = map(str.lower, shooting_data_against.columns) # make all the column names lower case       \n",
    "        \n",
    "        shooting_data_against.columns = shooting_data_against.columns.str.replace(' ', '_') # replace any spaces with an underscore\n",
    "                \n",
    "        shooting_data_against.rename(columns={'gls':'gls_a',\n",
    "                              'sh':'sh_a',\n",
    "                             'sot':'sot_a',\n",
    "                             'sot%':'sot%_a',\n",
    "                             'g/sh':'g/sh_a',\n",
    "                             'g/sot':'g/sot_a',\n",
    "                             'dist':'dist_a',\n",
    "                             'fk':'fk_a',\n",
    "                             'pk':'pk_a',\n",
    "                             'pkatt':'pkatt_a',\n",
    "                             'xg':'xg_a',\n",
    "                             'npxg':'npxg_a',\n",
    "                             'g-xg':'g-xg_a',\n",
    "                             'np:g-xg':'np:g-xg_a',}, inplace=True) # replace all column names so we know this is 'against' data           \n",
    "        \n",
    "        try: # set a try function \n",
    "            \n",
    "            # Merge all dataframes together selecting the relevant columns\n",
    "            matches_sdf = matches.merge(shooting_data_for[[\"date\", \"sh\", \"sot\"]], on=\"date\")\n",
    "            team_data = matches_sdf.merge(shooting_data_against[[\"date\", \"sh_a\", \"sot_a\"]], on=\"date\")\n",
    "            \n",
    "        except ValueError: # if error try again\n",
    "            \n",
    "            continue # otherwise continue to the rest of the code\n",
    "        \n",
    "        team_data[\"season\"] = year # create a new column to add the season\n",
    "        \n",
    "        team_data[\"team\"] = team_name # create a new column to add the team\n",
    "        \n",
    "        all_matches.append(team_data) # append team data to dataframe\n",
    "        \n",
    "        print(year, team_name)\n",
    "        \n",
    "        time.sleep(1) # rest for a second before moving on\n",
    "        \n",
    "\n",
    "match_df = pd.concat(all_matches) # concatinate into a dataframe\n",
    "\n",
    "match_df.rename(columns={'ga':'g_a',\n",
    "                         'gf':'g',\n",
    "                         'xga': 'xg_a'}, inplace=True) # rename columns for consistency\n",
    "\n",
    "match_df.to_csv(f\"../data/raw_EPL_data.csv\") # store this raw data as csv with time stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ee3a7-cb7a-4f82-8103-112b08143ff6",
   "metadata": {},
   "source": [
    "**Collect data on PL table position and points per game from previous seasons as an indication of team strength**\n",
    "\n",
    "One important aspect of prediction will be to tell how strong each team and opponent are. A proxy for this is their points per game across the previous season. We therefore need to collect data from the final standings of each PL table (including one for the season before our first season of full data).\n",
    "\n",
    "This section of code collects this data from the same website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cba5e-fdb0-4b61-aee3-08845ff5f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty object to store the premier league table position data\n",
    "pl_tables = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90a15f-f8f0-409a-ac2d-808b8014e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the URL with the premier league table \n",
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d2825-8437-4931-a9c8-18d01e47b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of years to collect PL table data (one more than stats perviously selected given we are using previous position)\n",
    "years = list(range(2022, 2015, -1))\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c15d2-3ead-4926-a685-b2fdb92748a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COLLECT PL TABLE POSITION DATA FOR THE SEASON\n",
    "for year in years:  \n",
    "    \n",
    "    pl_table_data = requests.get(standings_url) # send GET request to the standings URL\n",
    "    \n",
    "    pl_table = pd.read_html(pl_table_data.text)[0] # collect premier league table from each season \n",
    "    \n",
    "    pl_table.columns = map(str.lower, pl_table.columns) # make all the column names lower case\n",
    "    \n",
    "    pl_table[\"season\"] = year # add a season name to the PL table\n",
    "    \n",
    "    pl_table = pl_table[[\"season\", \"rk\", \"squad\", \"pts\", \"pts/mp\"]] # collect only relevant columns\n",
    "    \n",
    "    soup = BeautifulSoup(pl_table_data.text) # use BeautifulSoup to parse the text object received from the URL\n",
    "    \n",
    "    previous_season = soup.select(\"a.prev\")[0].get(\"href\") # collect the href link for the previous season\n",
    "    \n",
    "    standings_url = f\"https://fbref.com{previous_season}\" # complete the previous season url\n",
    "    \n",
    "    pl_tables.append(pl_table) # append pl position data to dataframe\n",
    "        \n",
    "    print(year) # print season year to help track progress\n",
    "    \n",
    "    time.sleep(1) # rest for a second before moving on\n",
    "\n",
    "pl_tables = pd.concat(pl_tables) # concatinate into a dataframe\n",
    "\n",
    "pl_tables.to_csv(f\"../data/raw_EPL_tables_data.csv\") # store as csv with time stamp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
